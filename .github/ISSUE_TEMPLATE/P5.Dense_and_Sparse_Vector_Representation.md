---
name: Phase05- Dense and Sparse Vector Representation for Text
about: Dense and Sparse Vector Representation for Text
title: Phase05- Dense and Sparse Vector Representation for Text
labels: ''
assignees: ''


---

- [x] Section 1: Types of data in the field of machine learning

  - [x] Providing an example for Categorical data.

    `[for example the whole time of a day that are night and morning]`

  - [x] Providing an example for Ordinal data.

    `[for example the name of the seasons is an ordinal data]`

  - [x] Providing an example for Numerical data.

    `[for example the price of pair of shoes is a numerical data ]`

- [x] Section 2: Sparse Token Representation 

  - [x] One-hot Token Representation

    - [x] One-hot strengths?

      `[It is good and a showable method refer indecis to vocabulary words ]`

    - [x] Weaknesses of One-hot?

      `[It is too large and have few informations and it is sparse]`

  - [x] Bag of Words Representation

    - [x] Strengths of Bag of Words?

      `[We can represent a sentence by a vector and do such operation that we want for example send it to a neural network and find datas from that]`

    - [x] Weaknesses of One-hot?

      `[If the number of vocabs grow we will have a very big vector and the computing will be so complex]`

    - [x] Compare One-hot and Bag of words.
      `[We use one hot encoding for encoding the words and represent them and it will be so sparse because all the elements are 0 except one but bag of word is for representing sentences and it is a litttle sparse and the elements are the frequency of repeating the words]`

  - [ ] Term-Frequency-Inverse-Document-Frequency (TFIDF)

    - [ ] What is Term-Frequency and why is it used?

      `[FILL HERE WITH RIGHT ANSWERS]`

    - [ ] What is Inverse-Document-Frequency and why is it used?

      `[FILL HERE WITH RIGHT ANSWERS]`

    - [ ] Compare One-hot, Bag of words and TFIDF.

      `[FILL HERE WITH RIGHT ANSWERS]`

- [ ] Section 2: Dense Token Representation 

  - [ ] Word2vec

    - [ ] How Word2vec represents tokens?

      `[FILL HERE WITH RIGHT ANSWERS]`

    - [ ] Why Word2vec is dense?

      `[FILL HERE WITH RIGHT ANSWERS]`

    
