{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "House_data_MLP_1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mahdiheidari721/AI-Internship/blob/master/House_data_MLP_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "yypXxj8RTswb"
      },
      "outputs": [],
      "source": [
        "# import libraries\n",
        "import torch\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://archive.ics.uci.edu/ml/machine-learning-databases/housing/housing.data"
      ],
      "metadata": {
        "id": "ocM_Tgglkfcq",
        "outputId": "1fe27f49-d8bb-4f64-c5b1-93a3a8bf9d49",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-08-15 14:16:41--  https://archive.ics.uci.edu/ml/machine-learning-databases/housing/housing.data\n",
            "Resolving archive.ics.uci.edu (archive.ics.uci.edu)... 128.195.10.252\n",
            "Connecting to archive.ics.uci.edu (archive.ics.uci.edu)|128.195.10.252|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 49082 (48K) [application/x-httpd-php]\n",
            "Saving to: ‘housing.data’\n",
            "\n",
            "housing.data        100%[===================>]  47.93K  --.-KB/s    in 0.1s    \n",
            "\n",
            "2022-08-15 14:16:41 (331 KB/s) - ‘housing.data’ saved [49082/49082]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error\n",
        "dataset = pd.read_csv('housing.data', sep=\"\\t\")\n",
        "y = dataset.iloc[:, -1].values\n",
        "A=[]\n",
        "for i in range(505) :\n",
        "  A.append(y[i].split())\n",
        "A=np.array(A)  \n",
        "X = A[:, :-1]\n",
        "y = A[:, -1]\n",
        "############# new lines #############\n",
        "y = y[:, np.newaxis]\n",
        "X = X.astype('float32')\n",
        "y = y.astype('float32')\n",
        "####################################\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)\n",
        "batch_size=16"
      ],
      "metadata": {
        "id": "Z-XddTyMbTaz"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "(X_train.shape, y_train.shape)"
      ],
      "metadata": {
        "id": "g5PfvE-akrXa",
        "outputId": "682466b8-e413-4be5-d424-1eda96f552a8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((404, 13), (404, 1))"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "############ data normaliztion #############\n",
        "max_vec_x_train = np.max(X_train, axis=0)\n",
        "min_vec_x_train = np.min(X_train, axis=0)\n",
        "\n",
        "max_vec_y_train = np.max(y_train, axis=0)\n",
        "min_vec_y_train = np.min(y_train, axis=0)\n",
        "\n",
        "\n",
        "X_train = (X_train - min_vec_x_train)/(max_vec_x_train - min_vec_x_train)\n",
        "X_test = (X_test - min_vec_x_train)/(max_vec_x_train - min_vec_x_train)\n",
        "\n",
        "y_train = (y_train - min_vec_y_train)/(max_vec_y_train - min_vec_y_train)\n",
        "y_test = (y_test - min_vec_y_train)/(max_vec_y_train - min_vec_y_train)"
      ],
      "metadata": {
        "id": "YZySoAV5vzIo"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# define the NN architecture\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        # number of hidden nodes in each layer (512)\n",
        "        hidden_1 = 32\n",
        "        hidden_2 = 32\n",
        "        hidden_3 = 1\n",
        "        # linear layer (784 -> hidden_1)\n",
        "        self.fc1 = nn.Linear(13, hidden_1)\n",
        "        self.fc2 = nn.Linear(hidden_1, hidden_2)\n",
        "        self.fc3 = nn.Linear(hidden_2, hidden_3)\n",
        "        # dropout layer (p=0.2)\n",
        "        # dropout prevents overfitting of data\n",
        "        self.dropout = nn.Dropout(0.2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # flatten image input\n",
        "        #x = x.view(-1, 13)\n",
        "        # add hidden layer, with relu activation function\n",
        "        x = F.tanh(self.fc1(x))\n",
        "        x = F.tanh(self.fc2(x))\n",
        "        # add dropout layer\n",
        "        #x = self.dropout(x)\n",
        "        x = F.sigmoid(self.fc3(x))\n",
        "        return x\n",
        "\n",
        "# initialize the NN\n",
        "model = Net()\n",
        "print(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4qVN9IkFUEZp",
        "outputId": "fa9d457d-f774-4823-8898-a6046bc1c65d"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Net(\n",
            "  (fc1): Linear(in_features=13, out_features=32, bias=True)\n",
            "  (fc2): Linear(in_features=32, out_features=32, bias=True)\n",
            "  (fc3): Linear(in_features=32, out_features=1, bias=True)\n",
            "  (dropout): Dropout(p=0.2, inplace=False)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# specify loss function (categorical cross-entropy)\n",
        "criterion = nn.MSELoss()\n",
        "\n",
        "# specify optimizer (stochastic gradient descent) and learning rate = 0.01\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)"
      ],
      "metadata": {
        "id": "yfSUaFNsUHIJ"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "xx=torch.from_numpy(X_train)\n",
        "yy=torch.from_numpy(y_train)\n",
        "xx1=torch.from_numpy(X_test)\n",
        "yy1=torch.from_numpy(y_test)\n",
        "\n",
        "model(xx[:10])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gXqrParms1E-",
        "outputId": "379ad371-c998-43f8-fa59-7616eba0eac8"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
            "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.8089],\n",
              "        [0.4343],\n",
              "        [0.3949],\n",
              "        [0.2892],\n",
              "        [0.0973],\n",
              "        [0.4340],\n",
              "        [0.4008],\n",
              "        [0.5045],\n",
              "        [0.2040],\n",
              "        [0.5758]], grad_fn=<SigmoidBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# number of epochs to train the model\n",
        "n_epochs = 1000\n",
        "\n",
        "#yy.unsqueeze(1)\n",
        "#xx.unsqueeze(1)\n",
        "model.train() # prep model for training\n",
        "for epoch in range(n_epochs):\n",
        "    # monitor training loss\n",
        "    train_loss = 0.0\n",
        "    counter = 0\n",
        "    \n",
        "    ###################\n",
        "    # train the model #\n",
        "    ###################\n",
        "\n",
        "    ############# new lines #############\n",
        "    for i in range(len(X_train)//batch_size) :\n",
        "      X_batch=X_train[i*batch_size:(i+1)*batch_size]\n",
        "      y_batch=y_train[i*batch_size:(i+1)*batch_size]\n",
        "    #####################################\n",
        "\n",
        "    #for i in range(len(y_train)):\n",
        "      optimizer.zero_grad()\n",
        "\n",
        "      ##modyfied\n",
        "      output = model(xx[i*batch_size:(i+1)*batch_size])\n",
        "      #yy.unsqueeze(1)\n",
        "      loss = criterion(output, yy[i*batch_size:(i+1)*batch_size])\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "      train_loss += loss.item()\n",
        "      counter += 1\n",
        "        # for data, target in train_loader:\n",
        "        # # clear the gradients of all optimized variables\n",
        "        # optimizer.zero_grad()\n",
        "        # # forward pass: compute predicted outputs by passing inputs to the model\n",
        "        # output = model(data)\n",
        "        # # calculate the loss\n",
        "        # loss = criterion(output, target)\n",
        "        # # backward pass: compute gradient of the loss with respect to model parameters\n",
        "        # loss.backward()\n",
        "        # # perform a single optimization step (parameter update)\n",
        "        # optimizer.step()\n",
        "        # # update running training loss\n",
        "        # train_loss += loss.item()*data.size(0) \n",
        "    # print training statistics \n",
        "    # calculate average loss over an epoch\n",
        "    train_loss = train_loss/counter\n",
        "\n",
        "    print('Epoch: {} \\tTraining Loss: {:.6f}'.format(\n",
        "        epoch+1, \n",
        "        train_loss\n",
        "        ))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MAnWCfKnUOoZ",
        "outputId": "855c158e-9a39-435d-c303-7df991d505b3"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
            "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1 \tTraining Loss: 0.010185\n",
            "Epoch: 2 \tTraining Loss: 0.010184\n",
            "Epoch: 3 \tTraining Loss: 0.010183\n",
            "Epoch: 4 \tTraining Loss: 0.010182\n",
            "Epoch: 5 \tTraining Loss: 0.010181\n",
            "Epoch: 6 \tTraining Loss: 0.010180\n",
            "Epoch: 7 \tTraining Loss: 0.010179\n",
            "Epoch: 8 \tTraining Loss: 0.010178\n",
            "Epoch: 9 \tTraining Loss: 0.010177\n",
            "Epoch: 10 \tTraining Loss: 0.010176\n",
            "Epoch: 11 \tTraining Loss: 0.010175\n",
            "Epoch: 12 \tTraining Loss: 0.010174\n",
            "Epoch: 13 \tTraining Loss: 0.010173\n",
            "Epoch: 14 \tTraining Loss: 0.010172\n",
            "Epoch: 15 \tTraining Loss: 0.010171\n",
            "Epoch: 16 \tTraining Loss: 0.010170\n",
            "Epoch: 17 \tTraining Loss: 0.010169\n",
            "Epoch: 18 \tTraining Loss: 0.010168\n",
            "Epoch: 19 \tTraining Loss: 0.010167\n",
            "Epoch: 20 \tTraining Loss: 0.010166\n",
            "Epoch: 21 \tTraining Loss: 0.010165\n",
            "Epoch: 22 \tTraining Loss: 0.010164\n",
            "Epoch: 23 \tTraining Loss: 0.010163\n",
            "Epoch: 24 \tTraining Loss: 0.010162\n",
            "Epoch: 25 \tTraining Loss: 0.010161\n",
            "Epoch: 26 \tTraining Loss: 0.010160\n",
            "Epoch: 27 \tTraining Loss: 0.010160\n",
            "Epoch: 28 \tTraining Loss: 0.010159\n",
            "Epoch: 29 \tTraining Loss: 0.010158\n",
            "Epoch: 30 \tTraining Loss: 0.010157\n",
            "Epoch: 31 \tTraining Loss: 0.010156\n",
            "Epoch: 32 \tTraining Loss: 0.010155\n",
            "Epoch: 33 \tTraining Loss: 0.010154\n",
            "Epoch: 34 \tTraining Loss: 0.010153\n",
            "Epoch: 35 \tTraining Loss: 0.010152\n",
            "Epoch: 36 \tTraining Loss: 0.010151\n",
            "Epoch: 37 \tTraining Loss: 0.010150\n",
            "Epoch: 38 \tTraining Loss: 0.010149\n",
            "Epoch: 39 \tTraining Loss: 0.010149\n",
            "Epoch: 40 \tTraining Loss: 0.010148\n",
            "Epoch: 41 \tTraining Loss: 0.010147\n",
            "Epoch: 42 \tTraining Loss: 0.010146\n",
            "Epoch: 43 \tTraining Loss: 0.010145\n",
            "Epoch: 44 \tTraining Loss: 0.010144\n",
            "Epoch: 45 \tTraining Loss: 0.010143\n",
            "Epoch: 46 \tTraining Loss: 0.010142\n",
            "Epoch: 47 \tTraining Loss: 0.010141\n",
            "Epoch: 48 \tTraining Loss: 0.010141\n",
            "Epoch: 49 \tTraining Loss: 0.010140\n",
            "Epoch: 50 \tTraining Loss: 0.010139\n",
            "Epoch: 51 \tTraining Loss: 0.010138\n",
            "Epoch: 52 \tTraining Loss: 0.010137\n",
            "Epoch: 53 \tTraining Loss: 0.010136\n",
            "Epoch: 54 \tTraining Loss: 0.010135\n",
            "Epoch: 55 \tTraining Loss: 0.010134\n",
            "Epoch: 56 \tTraining Loss: 0.010134\n",
            "Epoch: 57 \tTraining Loss: 0.010133\n",
            "Epoch: 58 \tTraining Loss: 0.010132\n",
            "Epoch: 59 \tTraining Loss: 0.010131\n",
            "Epoch: 60 \tTraining Loss: 0.010130\n",
            "Epoch: 61 \tTraining Loss: 0.010129\n",
            "Epoch: 62 \tTraining Loss: 0.010129\n",
            "Epoch: 63 \tTraining Loss: 0.010128\n",
            "Epoch: 64 \tTraining Loss: 0.010127\n",
            "Epoch: 65 \tTraining Loss: 0.010126\n",
            "Epoch: 66 \tTraining Loss: 0.010125\n",
            "Epoch: 67 \tTraining Loss: 0.010124\n",
            "Epoch: 68 \tTraining Loss: 0.010124\n",
            "Epoch: 69 \tTraining Loss: 0.010123\n",
            "Epoch: 70 \tTraining Loss: 0.010122\n",
            "Epoch: 71 \tTraining Loss: 0.010121\n",
            "Epoch: 72 \tTraining Loss: 0.010120\n",
            "Epoch: 73 \tTraining Loss: 0.010119\n",
            "Epoch: 74 \tTraining Loss: 0.010119\n",
            "Epoch: 75 \tTraining Loss: 0.010118\n",
            "Epoch: 76 \tTraining Loss: 0.010117\n",
            "Epoch: 77 \tTraining Loss: 0.010116\n",
            "Epoch: 78 \tTraining Loss: 0.010115\n",
            "Epoch: 79 \tTraining Loss: 0.010115\n",
            "Epoch: 80 \tTraining Loss: 0.010114\n",
            "Epoch: 81 \tTraining Loss: 0.010113\n",
            "Epoch: 82 \tTraining Loss: 0.010112\n",
            "Epoch: 83 \tTraining Loss: 0.010111\n",
            "Epoch: 84 \tTraining Loss: 0.010111\n",
            "Epoch: 85 \tTraining Loss: 0.010110\n",
            "Epoch: 86 \tTraining Loss: 0.010109\n",
            "Epoch: 87 \tTraining Loss: 0.010108\n",
            "Epoch: 88 \tTraining Loss: 0.010108\n",
            "Epoch: 89 \tTraining Loss: 0.010107\n",
            "Epoch: 90 \tTraining Loss: 0.010106\n",
            "Epoch: 91 \tTraining Loss: 0.010105\n",
            "Epoch: 92 \tTraining Loss: 0.010104\n",
            "Epoch: 93 \tTraining Loss: 0.010104\n",
            "Epoch: 94 \tTraining Loss: 0.010103\n",
            "Epoch: 95 \tTraining Loss: 0.010102\n",
            "Epoch: 96 \tTraining Loss: 0.010101\n",
            "Epoch: 97 \tTraining Loss: 0.010101\n",
            "Epoch: 98 \tTraining Loss: 0.010100\n",
            "Epoch: 99 \tTraining Loss: 0.010099\n",
            "Epoch: 100 \tTraining Loss: 0.010098\n",
            "Epoch: 101 \tTraining Loss: 0.010098\n",
            "Epoch: 102 \tTraining Loss: 0.010097\n",
            "Epoch: 103 \tTraining Loss: 0.010096\n",
            "Epoch: 104 \tTraining Loss: 0.010095\n",
            "Epoch: 105 \tTraining Loss: 0.010095\n",
            "Epoch: 106 \tTraining Loss: 0.010094\n",
            "Epoch: 107 \tTraining Loss: 0.010093\n",
            "Epoch: 108 \tTraining Loss: 0.010092\n",
            "Epoch: 109 \tTraining Loss: 0.010092\n",
            "Epoch: 110 \tTraining Loss: 0.010091\n",
            "Epoch: 111 \tTraining Loss: 0.010090\n",
            "Epoch: 112 \tTraining Loss: 0.010089\n",
            "Epoch: 113 \tTraining Loss: 0.010089\n",
            "Epoch: 114 \tTraining Loss: 0.010088\n",
            "Epoch: 115 \tTraining Loss: 0.010087\n",
            "Epoch: 116 \tTraining Loss: 0.010087\n",
            "Epoch: 117 \tTraining Loss: 0.010086\n",
            "Epoch: 118 \tTraining Loss: 0.010085\n",
            "Epoch: 119 \tTraining Loss: 0.010084\n",
            "Epoch: 120 \tTraining Loss: 0.010084\n",
            "Epoch: 121 \tTraining Loss: 0.010083\n",
            "Epoch: 122 \tTraining Loss: 0.010082\n",
            "Epoch: 123 \tTraining Loss: 0.010082\n",
            "Epoch: 124 \tTraining Loss: 0.010081\n",
            "Epoch: 125 \tTraining Loss: 0.010080\n",
            "Epoch: 126 \tTraining Loss: 0.010079\n",
            "Epoch: 127 \tTraining Loss: 0.010079\n",
            "Epoch: 128 \tTraining Loss: 0.010078\n",
            "Epoch: 129 \tTraining Loss: 0.010077\n",
            "Epoch: 130 \tTraining Loss: 0.010077\n",
            "Epoch: 131 \tTraining Loss: 0.010076\n",
            "Epoch: 132 \tTraining Loss: 0.010075\n",
            "Epoch: 133 \tTraining Loss: 0.010075\n",
            "Epoch: 134 \tTraining Loss: 0.010074\n",
            "Epoch: 135 \tTraining Loss: 0.010073\n",
            "Epoch: 136 \tTraining Loss: 0.010073\n",
            "Epoch: 137 \tTraining Loss: 0.010072\n",
            "Epoch: 138 \tTraining Loss: 0.010071\n",
            "Epoch: 139 \tTraining Loss: 0.010071\n",
            "Epoch: 140 \tTraining Loss: 0.010070\n",
            "Epoch: 141 \tTraining Loss: 0.010069\n",
            "Epoch: 142 \tTraining Loss: 0.010069\n",
            "Epoch: 143 \tTraining Loss: 0.010068\n",
            "Epoch: 144 \tTraining Loss: 0.010067\n",
            "Epoch: 145 \tTraining Loss: 0.010067\n",
            "Epoch: 146 \tTraining Loss: 0.010066\n",
            "Epoch: 147 \tTraining Loss: 0.010065\n",
            "Epoch: 148 \tTraining Loss: 0.010065\n",
            "Epoch: 149 \tTraining Loss: 0.010064\n",
            "Epoch: 150 \tTraining Loss: 0.010063\n",
            "Epoch: 151 \tTraining Loss: 0.010063\n",
            "Epoch: 152 \tTraining Loss: 0.010062\n",
            "Epoch: 153 \tTraining Loss: 0.010061\n",
            "Epoch: 154 \tTraining Loss: 0.010061\n",
            "Epoch: 155 \tTraining Loss: 0.010060\n",
            "Epoch: 156 \tTraining Loss: 0.010059\n",
            "Epoch: 157 \tTraining Loss: 0.010059\n",
            "Epoch: 158 \tTraining Loss: 0.010058\n",
            "Epoch: 159 \tTraining Loss: 0.010057\n",
            "Epoch: 160 \tTraining Loss: 0.010057\n",
            "Epoch: 161 \tTraining Loss: 0.010056\n",
            "Epoch: 162 \tTraining Loss: 0.010055\n",
            "Epoch: 163 \tTraining Loss: 0.010055\n",
            "Epoch: 164 \tTraining Loss: 0.010054\n",
            "Epoch: 165 \tTraining Loss: 0.010054\n",
            "Epoch: 166 \tTraining Loss: 0.010053\n",
            "Epoch: 167 \tTraining Loss: 0.010052\n",
            "Epoch: 168 \tTraining Loss: 0.010052\n",
            "Epoch: 169 \tTraining Loss: 0.010051\n",
            "Epoch: 170 \tTraining Loss: 0.010050\n",
            "Epoch: 171 \tTraining Loss: 0.010050\n",
            "Epoch: 172 \tTraining Loss: 0.010049\n",
            "Epoch: 173 \tTraining Loss: 0.010049\n",
            "Epoch: 174 \tTraining Loss: 0.010048\n",
            "Epoch: 175 \tTraining Loss: 0.010047\n",
            "Epoch: 176 \tTraining Loss: 0.010047\n",
            "Epoch: 177 \tTraining Loss: 0.010046\n",
            "Epoch: 178 \tTraining Loss: 0.010045\n",
            "Epoch: 179 \tTraining Loss: 0.010045\n",
            "Epoch: 180 \tTraining Loss: 0.010044\n",
            "Epoch: 181 \tTraining Loss: 0.010044\n",
            "Epoch: 182 \tTraining Loss: 0.010043\n",
            "Epoch: 183 \tTraining Loss: 0.010042\n",
            "Epoch: 184 \tTraining Loss: 0.010042\n",
            "Epoch: 185 \tTraining Loss: 0.010041\n",
            "Epoch: 186 \tTraining Loss: 0.010041\n",
            "Epoch: 187 \tTraining Loss: 0.010040\n",
            "Epoch: 188 \tTraining Loss: 0.010039\n",
            "Epoch: 189 \tTraining Loss: 0.010039\n",
            "Epoch: 190 \tTraining Loss: 0.010038\n",
            "Epoch: 191 \tTraining Loss: 0.010038\n",
            "Epoch: 192 \tTraining Loss: 0.010037\n",
            "Epoch: 193 \tTraining Loss: 0.010036\n",
            "Epoch: 194 \tTraining Loss: 0.010036\n",
            "Epoch: 195 \tTraining Loss: 0.010035\n",
            "Epoch: 196 \tTraining Loss: 0.010035\n",
            "Epoch: 197 \tTraining Loss: 0.010034\n",
            "Epoch: 198 \tTraining Loss: 0.010033\n",
            "Epoch: 199 \tTraining Loss: 0.010033\n",
            "Epoch: 200 \tTraining Loss: 0.010032\n",
            "Epoch: 201 \tTraining Loss: 0.010032\n",
            "Epoch: 202 \tTraining Loss: 0.010031\n",
            "Epoch: 203 \tTraining Loss: 0.010031\n",
            "Epoch: 204 \tTraining Loss: 0.010030\n",
            "Epoch: 205 \tTraining Loss: 0.010029\n",
            "Epoch: 206 \tTraining Loss: 0.010029\n",
            "Epoch: 207 \tTraining Loss: 0.010028\n",
            "Epoch: 208 \tTraining Loss: 0.010028\n",
            "Epoch: 209 \tTraining Loss: 0.010027\n",
            "Epoch: 210 \tTraining Loss: 0.010026\n",
            "Epoch: 211 \tTraining Loss: 0.010026\n",
            "Epoch: 212 \tTraining Loss: 0.010025\n",
            "Epoch: 213 \tTraining Loss: 0.010025\n",
            "Epoch: 214 \tTraining Loss: 0.010024\n",
            "Epoch: 215 \tTraining Loss: 0.010024\n",
            "Epoch: 216 \tTraining Loss: 0.010023\n",
            "Epoch: 217 \tTraining Loss: 0.010023\n",
            "Epoch: 218 \tTraining Loss: 0.010022\n",
            "Epoch: 219 \tTraining Loss: 0.010021\n",
            "Epoch: 220 \tTraining Loss: 0.010021\n",
            "Epoch: 221 \tTraining Loss: 0.010020\n",
            "Epoch: 222 \tTraining Loss: 0.010020\n",
            "Epoch: 223 \tTraining Loss: 0.010019\n",
            "Epoch: 224 \tTraining Loss: 0.010019\n",
            "Epoch: 225 \tTraining Loss: 0.010018\n",
            "Epoch: 226 \tTraining Loss: 0.010018\n",
            "Epoch: 227 \tTraining Loss: 0.010017\n",
            "Epoch: 228 \tTraining Loss: 0.010016\n",
            "Epoch: 229 \tTraining Loss: 0.010016\n",
            "Epoch: 230 \tTraining Loss: 0.010015\n",
            "Epoch: 231 \tTraining Loss: 0.010015\n",
            "Epoch: 232 \tTraining Loss: 0.010014\n",
            "Epoch: 233 \tTraining Loss: 0.010014\n",
            "Epoch: 234 \tTraining Loss: 0.010013\n",
            "Epoch: 235 \tTraining Loss: 0.010013\n",
            "Epoch: 236 \tTraining Loss: 0.010012\n",
            "Epoch: 237 \tTraining Loss: 0.010012\n",
            "Epoch: 238 \tTraining Loss: 0.010011\n",
            "Epoch: 239 \tTraining Loss: 0.010010\n",
            "Epoch: 240 \tTraining Loss: 0.010010\n",
            "Epoch: 241 \tTraining Loss: 0.010009\n",
            "Epoch: 242 \tTraining Loss: 0.010009\n",
            "Epoch: 243 \tTraining Loss: 0.010008\n",
            "Epoch: 244 \tTraining Loss: 0.010008\n",
            "Epoch: 245 \tTraining Loss: 0.010007\n",
            "Epoch: 246 \tTraining Loss: 0.010007\n",
            "Epoch: 247 \tTraining Loss: 0.010006\n",
            "Epoch: 248 \tTraining Loss: 0.010006\n",
            "Epoch: 249 \tTraining Loss: 0.010005\n",
            "Epoch: 250 \tTraining Loss: 0.010005\n",
            "Epoch: 251 \tTraining Loss: 0.010004\n",
            "Epoch: 252 \tTraining Loss: 0.010004\n",
            "Epoch: 253 \tTraining Loss: 0.010003\n",
            "Epoch: 254 \tTraining Loss: 0.010003\n",
            "Epoch: 255 \tTraining Loss: 0.010002\n",
            "Epoch: 256 \tTraining Loss: 0.010001\n",
            "Epoch: 257 \tTraining Loss: 0.010001\n",
            "Epoch: 258 \tTraining Loss: 0.010000\n",
            "Epoch: 259 \tTraining Loss: 0.010000\n",
            "Epoch: 260 \tTraining Loss: 0.009999\n",
            "Epoch: 261 \tTraining Loss: 0.009999\n",
            "Epoch: 262 \tTraining Loss: 0.009998\n",
            "Epoch: 263 \tTraining Loss: 0.009998\n",
            "Epoch: 264 \tTraining Loss: 0.009997\n",
            "Epoch: 265 \tTraining Loss: 0.009997\n",
            "Epoch: 266 \tTraining Loss: 0.009996\n",
            "Epoch: 267 \tTraining Loss: 0.009996\n",
            "Epoch: 268 \tTraining Loss: 0.009995\n",
            "Epoch: 269 \tTraining Loss: 0.009995\n",
            "Epoch: 270 \tTraining Loss: 0.009994\n",
            "Epoch: 271 \tTraining Loss: 0.009994\n",
            "Epoch: 272 \tTraining Loss: 0.009993\n",
            "Epoch: 273 \tTraining Loss: 0.009993\n",
            "Epoch: 274 \tTraining Loss: 0.009992\n",
            "Epoch: 275 \tTraining Loss: 0.009992\n",
            "Epoch: 276 \tTraining Loss: 0.009991\n",
            "Epoch: 277 \tTraining Loss: 0.009991\n",
            "Epoch: 278 \tTraining Loss: 0.009990\n",
            "Epoch: 279 \tTraining Loss: 0.009990\n",
            "Epoch: 280 \tTraining Loss: 0.009989\n",
            "Epoch: 281 \tTraining Loss: 0.009989\n",
            "Epoch: 282 \tTraining Loss: 0.009988\n",
            "Epoch: 283 \tTraining Loss: 0.009988\n",
            "Epoch: 284 \tTraining Loss: 0.009987\n",
            "Epoch: 285 \tTraining Loss: 0.009987\n",
            "Epoch: 286 \tTraining Loss: 0.009986\n",
            "Epoch: 287 \tTraining Loss: 0.009986\n",
            "Epoch: 288 \tTraining Loss: 0.009985\n",
            "Epoch: 289 \tTraining Loss: 0.009985\n",
            "Epoch: 290 \tTraining Loss: 0.009984\n",
            "Epoch: 291 \tTraining Loss: 0.009984\n",
            "Epoch: 292 \tTraining Loss: 0.009983\n",
            "Epoch: 293 \tTraining Loss: 0.009983\n",
            "Epoch: 294 \tTraining Loss: 0.009982\n",
            "Epoch: 295 \tTraining Loss: 0.009982\n",
            "Epoch: 296 \tTraining Loss: 0.009982\n",
            "Epoch: 297 \tTraining Loss: 0.009981\n",
            "Epoch: 298 \tTraining Loss: 0.009981\n",
            "Epoch: 299 \tTraining Loss: 0.009980\n",
            "Epoch: 300 \tTraining Loss: 0.009980\n",
            "Epoch: 301 \tTraining Loss: 0.009979\n",
            "Epoch: 302 \tTraining Loss: 0.009979\n",
            "Epoch: 303 \tTraining Loss: 0.009978\n",
            "Epoch: 304 \tTraining Loss: 0.009978\n",
            "Epoch: 305 \tTraining Loss: 0.009977\n",
            "Epoch: 306 \tTraining Loss: 0.009977\n",
            "Epoch: 307 \tTraining Loss: 0.009976\n",
            "Epoch: 308 \tTraining Loss: 0.009976\n",
            "Epoch: 309 \tTraining Loss: 0.009975\n",
            "Epoch: 310 \tTraining Loss: 0.009975\n",
            "Epoch: 311 \tTraining Loss: 0.009974\n",
            "Epoch: 312 \tTraining Loss: 0.009974\n",
            "Epoch: 313 \tTraining Loss: 0.009973\n",
            "Epoch: 314 \tTraining Loss: 0.009973\n",
            "Epoch: 315 \tTraining Loss: 0.009973\n",
            "Epoch: 316 \tTraining Loss: 0.009972\n",
            "Epoch: 317 \tTraining Loss: 0.009972\n",
            "Epoch: 318 \tTraining Loss: 0.009971\n",
            "Epoch: 319 \tTraining Loss: 0.009971\n",
            "Epoch: 320 \tTraining Loss: 0.009970\n",
            "Epoch: 321 \tTraining Loss: 0.009970\n",
            "Epoch: 322 \tTraining Loss: 0.009969\n",
            "Epoch: 323 \tTraining Loss: 0.009969\n",
            "Epoch: 324 \tTraining Loss: 0.009968\n",
            "Epoch: 325 \tTraining Loss: 0.009968\n",
            "Epoch: 326 \tTraining Loss: 0.009967\n",
            "Epoch: 327 \tTraining Loss: 0.009967\n",
            "Epoch: 328 \tTraining Loss: 0.009967\n",
            "Epoch: 329 \tTraining Loss: 0.009966\n",
            "Epoch: 330 \tTraining Loss: 0.009966\n",
            "Epoch: 331 \tTraining Loss: 0.009965\n",
            "Epoch: 332 \tTraining Loss: 0.009965\n",
            "Epoch: 333 \tTraining Loss: 0.009964\n",
            "Epoch: 334 \tTraining Loss: 0.009964\n",
            "Epoch: 335 \tTraining Loss: 0.009963\n",
            "Epoch: 336 \tTraining Loss: 0.009963\n",
            "Epoch: 337 \tTraining Loss: 0.009962\n",
            "Epoch: 338 \tTraining Loss: 0.009962\n",
            "Epoch: 339 \tTraining Loss: 0.009962\n",
            "Epoch: 340 \tTraining Loss: 0.009961\n",
            "Epoch: 341 \tTraining Loss: 0.009961\n",
            "Epoch: 342 \tTraining Loss: 0.009960\n",
            "Epoch: 343 \tTraining Loss: 0.009960\n",
            "Epoch: 344 \tTraining Loss: 0.009959\n",
            "Epoch: 345 \tTraining Loss: 0.009959\n",
            "Epoch: 346 \tTraining Loss: 0.009958\n",
            "Epoch: 347 \tTraining Loss: 0.009958\n",
            "Epoch: 348 \tTraining Loss: 0.009958\n",
            "Epoch: 349 \tTraining Loss: 0.009957\n",
            "Epoch: 350 \tTraining Loss: 0.009957\n",
            "Epoch: 351 \tTraining Loss: 0.009956\n",
            "Epoch: 352 \tTraining Loss: 0.009956\n",
            "Epoch: 353 \tTraining Loss: 0.009955\n",
            "Epoch: 354 \tTraining Loss: 0.009955\n",
            "Epoch: 355 \tTraining Loss: 0.009955\n",
            "Epoch: 356 \tTraining Loss: 0.009954\n",
            "Epoch: 357 \tTraining Loss: 0.009954\n",
            "Epoch: 358 \tTraining Loss: 0.009953\n",
            "Epoch: 359 \tTraining Loss: 0.009953\n",
            "Epoch: 360 \tTraining Loss: 0.009952\n",
            "Epoch: 361 \tTraining Loss: 0.009952\n",
            "Epoch: 362 \tTraining Loss: 0.009951\n",
            "Epoch: 363 \tTraining Loss: 0.009951\n",
            "Epoch: 364 \tTraining Loss: 0.009951\n",
            "Epoch: 365 \tTraining Loss: 0.009950\n",
            "Epoch: 366 \tTraining Loss: 0.009950\n",
            "Epoch: 367 \tTraining Loss: 0.009949\n",
            "Epoch: 368 \tTraining Loss: 0.009949\n",
            "Epoch: 369 \tTraining Loss: 0.009948\n",
            "Epoch: 370 \tTraining Loss: 0.009948\n",
            "Epoch: 371 \tTraining Loss: 0.009948\n",
            "Epoch: 372 \tTraining Loss: 0.009947\n",
            "Epoch: 373 \tTraining Loss: 0.009947\n",
            "Epoch: 374 \tTraining Loss: 0.009946\n",
            "Epoch: 375 \tTraining Loss: 0.009946\n",
            "Epoch: 376 \tTraining Loss: 0.009946\n",
            "Epoch: 377 \tTraining Loss: 0.009945\n",
            "Epoch: 378 \tTraining Loss: 0.009945\n",
            "Epoch: 379 \tTraining Loss: 0.009944\n",
            "Epoch: 380 \tTraining Loss: 0.009944\n",
            "Epoch: 381 \tTraining Loss: 0.009943\n",
            "Epoch: 382 \tTraining Loss: 0.009943\n",
            "Epoch: 383 \tTraining Loss: 0.009943\n",
            "Epoch: 384 \tTraining Loss: 0.009942\n",
            "Epoch: 385 \tTraining Loss: 0.009942\n",
            "Epoch: 386 \tTraining Loss: 0.009941\n",
            "Epoch: 387 \tTraining Loss: 0.009941\n",
            "Epoch: 388 \tTraining Loss: 0.009940\n",
            "Epoch: 389 \tTraining Loss: 0.009940\n",
            "Epoch: 390 \tTraining Loss: 0.009940\n",
            "Epoch: 391 \tTraining Loss: 0.009939\n",
            "Epoch: 392 \tTraining Loss: 0.009939\n",
            "Epoch: 393 \tTraining Loss: 0.009938\n",
            "Epoch: 394 \tTraining Loss: 0.009938\n",
            "Epoch: 395 \tTraining Loss: 0.009938\n",
            "Epoch: 396 \tTraining Loss: 0.009937\n",
            "Epoch: 397 \tTraining Loss: 0.009937\n",
            "Epoch: 398 \tTraining Loss: 0.009936\n",
            "Epoch: 399 \tTraining Loss: 0.009936\n",
            "Epoch: 400 \tTraining Loss: 0.009936\n",
            "Epoch: 401 \tTraining Loss: 0.009935\n",
            "Epoch: 402 \tTraining Loss: 0.009935\n",
            "Epoch: 403 \tTraining Loss: 0.009934\n",
            "Epoch: 404 \tTraining Loss: 0.009934\n",
            "Epoch: 405 \tTraining Loss: 0.009933\n",
            "Epoch: 406 \tTraining Loss: 0.009933\n",
            "Epoch: 407 \tTraining Loss: 0.009933\n",
            "Epoch: 408 \tTraining Loss: 0.009932\n",
            "Epoch: 409 \tTraining Loss: 0.009932\n",
            "Epoch: 410 \tTraining Loss: 0.009931\n",
            "Epoch: 411 \tTraining Loss: 0.009931\n",
            "Epoch: 412 \tTraining Loss: 0.009931\n",
            "Epoch: 413 \tTraining Loss: 0.009930\n",
            "Epoch: 414 \tTraining Loss: 0.009930\n",
            "Epoch: 415 \tTraining Loss: 0.009929\n",
            "Epoch: 416 \tTraining Loss: 0.009929\n",
            "Epoch: 417 \tTraining Loss: 0.009929\n",
            "Epoch: 418 \tTraining Loss: 0.009928\n",
            "Epoch: 419 \tTraining Loss: 0.009928\n",
            "Epoch: 420 \tTraining Loss: 0.009927\n",
            "Epoch: 421 \tTraining Loss: 0.009927\n",
            "Epoch: 422 \tTraining Loss: 0.009927\n",
            "Epoch: 423 \tTraining Loss: 0.009926\n",
            "Epoch: 424 \tTraining Loss: 0.009926\n",
            "Epoch: 425 \tTraining Loss: 0.009925\n",
            "Epoch: 426 \tTraining Loss: 0.009925\n",
            "Epoch: 427 \tTraining Loss: 0.009925\n",
            "Epoch: 428 \tTraining Loss: 0.009924\n",
            "Epoch: 429 \tTraining Loss: 0.009924\n",
            "Epoch: 430 \tTraining Loss: 0.009924\n",
            "Epoch: 431 \tTraining Loss: 0.009923\n",
            "Epoch: 432 \tTraining Loss: 0.009923\n",
            "Epoch: 433 \tTraining Loss: 0.009922\n",
            "Epoch: 434 \tTraining Loss: 0.009922\n",
            "Epoch: 435 \tTraining Loss: 0.009922\n",
            "Epoch: 436 \tTraining Loss: 0.009921\n",
            "Epoch: 437 \tTraining Loss: 0.009921\n",
            "Epoch: 438 \tTraining Loss: 0.009920\n",
            "Epoch: 439 \tTraining Loss: 0.009920\n",
            "Epoch: 440 \tTraining Loss: 0.009920\n",
            "Epoch: 441 \tTraining Loss: 0.009919\n",
            "Epoch: 442 \tTraining Loss: 0.009919\n",
            "Epoch: 443 \tTraining Loss: 0.009918\n",
            "Epoch: 444 \tTraining Loss: 0.009918\n",
            "Epoch: 445 \tTraining Loss: 0.009918\n",
            "Epoch: 446 \tTraining Loss: 0.009917\n",
            "Epoch: 447 \tTraining Loss: 0.009917\n",
            "Epoch: 448 \tTraining Loss: 0.009917\n",
            "Epoch: 449 \tTraining Loss: 0.009916\n",
            "Epoch: 450 \tTraining Loss: 0.009916\n",
            "Epoch: 451 \tTraining Loss: 0.009915\n",
            "Epoch: 452 \tTraining Loss: 0.009915\n",
            "Epoch: 453 \tTraining Loss: 0.009915\n",
            "Epoch: 454 \tTraining Loss: 0.009914\n",
            "Epoch: 455 \tTraining Loss: 0.009914\n",
            "Epoch: 456 \tTraining Loss: 0.009913\n",
            "Epoch: 457 \tTraining Loss: 0.009913\n",
            "Epoch: 458 \tTraining Loss: 0.009913\n",
            "Epoch: 459 \tTraining Loss: 0.009912\n",
            "Epoch: 460 \tTraining Loss: 0.009912\n",
            "Epoch: 461 \tTraining Loss: 0.009912\n",
            "Epoch: 462 \tTraining Loss: 0.009911\n",
            "Epoch: 463 \tTraining Loss: 0.009911\n",
            "Epoch: 464 \tTraining Loss: 0.009910\n",
            "Epoch: 465 \tTraining Loss: 0.009910\n",
            "Epoch: 466 \tTraining Loss: 0.009910\n",
            "Epoch: 467 \tTraining Loss: 0.009909\n",
            "Epoch: 468 \tTraining Loss: 0.009909\n",
            "Epoch: 469 \tTraining Loss: 0.009909\n",
            "Epoch: 470 \tTraining Loss: 0.009908\n",
            "Epoch: 471 \tTraining Loss: 0.009908\n",
            "Epoch: 472 \tTraining Loss: 0.009907\n",
            "Epoch: 473 \tTraining Loss: 0.009907\n",
            "Epoch: 474 \tTraining Loss: 0.009907\n",
            "Epoch: 475 \tTraining Loss: 0.009906\n",
            "Epoch: 476 \tTraining Loss: 0.009906\n",
            "Epoch: 477 \tTraining Loss: 0.009906\n",
            "Epoch: 478 \tTraining Loss: 0.009905\n",
            "Epoch: 479 \tTraining Loss: 0.009905\n",
            "Epoch: 480 \tTraining Loss: 0.009904\n",
            "Epoch: 481 \tTraining Loss: 0.009904\n",
            "Epoch: 482 \tTraining Loss: 0.009904\n",
            "Epoch: 483 \tTraining Loss: 0.009903\n",
            "Epoch: 484 \tTraining Loss: 0.009903\n",
            "Epoch: 485 \tTraining Loss: 0.009903\n",
            "Epoch: 486 \tTraining Loss: 0.009902\n",
            "Epoch: 487 \tTraining Loss: 0.009902\n",
            "Epoch: 488 \tTraining Loss: 0.009901\n",
            "Epoch: 489 \tTraining Loss: 0.009901\n",
            "Epoch: 490 \tTraining Loss: 0.009901\n",
            "Epoch: 491 \tTraining Loss: 0.009900\n",
            "Epoch: 492 \tTraining Loss: 0.009900\n",
            "Epoch: 493 \tTraining Loss: 0.009900\n",
            "Epoch: 494 \tTraining Loss: 0.009899\n",
            "Epoch: 495 \tTraining Loss: 0.009899\n",
            "Epoch: 496 \tTraining Loss: 0.009899\n",
            "Epoch: 497 \tTraining Loss: 0.009898\n",
            "Epoch: 498 \tTraining Loss: 0.009898\n",
            "Epoch: 499 \tTraining Loss: 0.009897\n",
            "Epoch: 500 \tTraining Loss: 0.009897\n",
            "Epoch: 501 \tTraining Loss: 0.009897\n",
            "Epoch: 502 \tTraining Loss: 0.009896\n",
            "Epoch: 503 \tTraining Loss: 0.009896\n",
            "Epoch: 504 \tTraining Loss: 0.009896\n",
            "Epoch: 505 \tTraining Loss: 0.009895\n",
            "Epoch: 506 \tTraining Loss: 0.009895\n",
            "Epoch: 507 \tTraining Loss: 0.009895\n",
            "Epoch: 508 \tTraining Loss: 0.009894\n",
            "Epoch: 509 \tTraining Loss: 0.009894\n",
            "Epoch: 510 \tTraining Loss: 0.009893\n",
            "Epoch: 511 \tTraining Loss: 0.009893\n",
            "Epoch: 512 \tTraining Loss: 0.009893\n",
            "Epoch: 513 \tTraining Loss: 0.009892\n",
            "Epoch: 514 \tTraining Loss: 0.009892\n",
            "Epoch: 515 \tTraining Loss: 0.009892\n",
            "Epoch: 516 \tTraining Loss: 0.009891\n",
            "Epoch: 517 \tTraining Loss: 0.009891\n",
            "Epoch: 518 \tTraining Loss: 0.009891\n",
            "Epoch: 519 \tTraining Loss: 0.009890\n",
            "Epoch: 520 \tTraining Loss: 0.009890\n",
            "Epoch: 521 \tTraining Loss: 0.009890\n",
            "Epoch: 522 \tTraining Loss: 0.009889\n",
            "Epoch: 523 \tTraining Loss: 0.009889\n",
            "Epoch: 524 \tTraining Loss: 0.009888\n",
            "Epoch: 525 \tTraining Loss: 0.009888\n",
            "Epoch: 526 \tTraining Loss: 0.009888\n",
            "Epoch: 527 \tTraining Loss: 0.009887\n",
            "Epoch: 528 \tTraining Loss: 0.009887\n",
            "Epoch: 529 \tTraining Loss: 0.009887\n",
            "Epoch: 530 \tTraining Loss: 0.009886\n",
            "Epoch: 531 \tTraining Loss: 0.009886\n",
            "Epoch: 532 \tTraining Loss: 0.009886\n",
            "Epoch: 533 \tTraining Loss: 0.009885\n",
            "Epoch: 534 \tTraining Loss: 0.009885\n",
            "Epoch: 535 \tTraining Loss: 0.009885\n",
            "Epoch: 536 \tTraining Loss: 0.009884\n",
            "Epoch: 537 \tTraining Loss: 0.009884\n",
            "Epoch: 538 \tTraining Loss: 0.009884\n",
            "Epoch: 539 \tTraining Loss: 0.009883\n",
            "Epoch: 540 \tTraining Loss: 0.009883\n",
            "Epoch: 541 \tTraining Loss: 0.009883\n",
            "Epoch: 542 \tTraining Loss: 0.009882\n",
            "Epoch: 543 \tTraining Loss: 0.009882\n",
            "Epoch: 544 \tTraining Loss: 0.009881\n",
            "Epoch: 545 \tTraining Loss: 0.009881\n",
            "Epoch: 546 \tTraining Loss: 0.009881\n",
            "Epoch: 547 \tTraining Loss: 0.009880\n",
            "Epoch: 548 \tTraining Loss: 0.009880\n",
            "Epoch: 549 \tTraining Loss: 0.009880\n",
            "Epoch: 550 \tTraining Loss: 0.009879\n",
            "Epoch: 551 \tTraining Loss: 0.009879\n",
            "Epoch: 552 \tTraining Loss: 0.009879\n",
            "Epoch: 553 \tTraining Loss: 0.009878\n",
            "Epoch: 554 \tTraining Loss: 0.009878\n",
            "Epoch: 555 \tTraining Loss: 0.009878\n",
            "Epoch: 556 \tTraining Loss: 0.009877\n",
            "Epoch: 557 \tTraining Loss: 0.009877\n",
            "Epoch: 558 \tTraining Loss: 0.009877\n",
            "Epoch: 559 \tTraining Loss: 0.009876\n",
            "Epoch: 560 \tTraining Loss: 0.009876\n",
            "Epoch: 561 \tTraining Loss: 0.009876\n",
            "Epoch: 562 \tTraining Loss: 0.009875\n",
            "Epoch: 563 \tTraining Loss: 0.009875\n",
            "Epoch: 564 \tTraining Loss: 0.009875\n",
            "Epoch: 565 \tTraining Loss: 0.009874\n",
            "Epoch: 566 \tTraining Loss: 0.009874\n",
            "Epoch: 567 \tTraining Loss: 0.009874\n",
            "Epoch: 568 \tTraining Loss: 0.009873\n",
            "Epoch: 569 \tTraining Loss: 0.009873\n",
            "Epoch: 570 \tTraining Loss: 0.009873\n",
            "Epoch: 571 \tTraining Loss: 0.009872\n",
            "Epoch: 572 \tTraining Loss: 0.009872\n",
            "Epoch: 573 \tTraining Loss: 0.009872\n",
            "Epoch: 574 \tTraining Loss: 0.009871\n",
            "Epoch: 575 \tTraining Loss: 0.009871\n",
            "Epoch: 576 \tTraining Loss: 0.009870\n",
            "Epoch: 577 \tTraining Loss: 0.009870\n",
            "Epoch: 578 \tTraining Loss: 0.009870\n",
            "Epoch: 579 \tTraining Loss: 0.009869\n",
            "Epoch: 580 \tTraining Loss: 0.009869\n",
            "Epoch: 581 \tTraining Loss: 0.009869\n",
            "Epoch: 582 \tTraining Loss: 0.009868\n",
            "Epoch: 583 \tTraining Loss: 0.009868\n",
            "Epoch: 584 \tTraining Loss: 0.009868\n",
            "Epoch: 585 \tTraining Loss: 0.009867\n",
            "Epoch: 586 \tTraining Loss: 0.009867\n",
            "Epoch: 587 \tTraining Loss: 0.009867\n",
            "Epoch: 588 \tTraining Loss: 0.009866\n",
            "Epoch: 589 \tTraining Loss: 0.009866\n",
            "Epoch: 590 \tTraining Loss: 0.009866\n",
            "Epoch: 591 \tTraining Loss: 0.009865\n",
            "Epoch: 592 \tTraining Loss: 0.009865\n",
            "Epoch: 593 \tTraining Loss: 0.009865\n",
            "Epoch: 594 \tTraining Loss: 0.009864\n",
            "Epoch: 595 \tTraining Loss: 0.009864\n",
            "Epoch: 596 \tTraining Loss: 0.009864\n",
            "Epoch: 597 \tTraining Loss: 0.009863\n",
            "Epoch: 598 \tTraining Loss: 0.009863\n",
            "Epoch: 599 \tTraining Loss: 0.009863\n",
            "Epoch: 600 \tTraining Loss: 0.009862\n",
            "Epoch: 601 \tTraining Loss: 0.009862\n",
            "Epoch: 602 \tTraining Loss: 0.009862\n",
            "Epoch: 603 \tTraining Loss: 0.009861\n",
            "Epoch: 604 \tTraining Loss: 0.009861\n",
            "Epoch: 605 \tTraining Loss: 0.009861\n",
            "Epoch: 606 \tTraining Loss: 0.009860\n",
            "Epoch: 607 \tTraining Loss: 0.009860\n",
            "Epoch: 608 \tTraining Loss: 0.009860\n",
            "Epoch: 609 \tTraining Loss: 0.009860\n",
            "Epoch: 610 \tTraining Loss: 0.009859\n",
            "Epoch: 611 \tTraining Loss: 0.009859\n",
            "Epoch: 612 \tTraining Loss: 0.009859\n",
            "Epoch: 613 \tTraining Loss: 0.009858\n",
            "Epoch: 614 \tTraining Loss: 0.009858\n",
            "Epoch: 615 \tTraining Loss: 0.009858\n",
            "Epoch: 616 \tTraining Loss: 0.009857\n",
            "Epoch: 617 \tTraining Loss: 0.009857\n",
            "Epoch: 618 \tTraining Loss: 0.009857\n",
            "Epoch: 619 \tTraining Loss: 0.009856\n",
            "Epoch: 620 \tTraining Loss: 0.009856\n",
            "Epoch: 621 \tTraining Loss: 0.009856\n",
            "Epoch: 622 \tTraining Loss: 0.009855\n",
            "Epoch: 623 \tTraining Loss: 0.009855\n",
            "Epoch: 624 \tTraining Loss: 0.009855\n",
            "Epoch: 625 \tTraining Loss: 0.009854\n",
            "Epoch: 626 \tTraining Loss: 0.009854\n",
            "Epoch: 627 \tTraining Loss: 0.009854\n",
            "Epoch: 628 \tTraining Loss: 0.009853\n",
            "Epoch: 629 \tTraining Loss: 0.009853\n",
            "Epoch: 630 \tTraining Loss: 0.009853\n",
            "Epoch: 631 \tTraining Loss: 0.009852\n",
            "Epoch: 632 \tTraining Loss: 0.009852\n",
            "Epoch: 633 \tTraining Loss: 0.009852\n",
            "Epoch: 634 \tTraining Loss: 0.009851\n",
            "Epoch: 635 \tTraining Loss: 0.009851\n",
            "Epoch: 636 \tTraining Loss: 0.009851\n",
            "Epoch: 637 \tTraining Loss: 0.009850\n",
            "Epoch: 638 \tTraining Loss: 0.009850\n",
            "Epoch: 639 \tTraining Loss: 0.009850\n",
            "Epoch: 640 \tTraining Loss: 0.009849\n",
            "Epoch: 641 \tTraining Loss: 0.009849\n",
            "Epoch: 642 \tTraining Loss: 0.009849\n",
            "Epoch: 643 \tTraining Loss: 0.009848\n",
            "Epoch: 644 \tTraining Loss: 0.009848\n",
            "Epoch: 645 \tTraining Loss: 0.009848\n",
            "Epoch: 646 \tTraining Loss: 0.009848\n",
            "Epoch: 647 \tTraining Loss: 0.009847\n",
            "Epoch: 648 \tTraining Loss: 0.009847\n",
            "Epoch: 649 \tTraining Loss: 0.009847\n",
            "Epoch: 650 \tTraining Loss: 0.009846\n",
            "Epoch: 651 \tTraining Loss: 0.009846\n",
            "Epoch: 652 \tTraining Loss: 0.009846\n",
            "Epoch: 653 \tTraining Loss: 0.009845\n",
            "Epoch: 654 \tTraining Loss: 0.009845\n",
            "Epoch: 655 \tTraining Loss: 0.009845\n",
            "Epoch: 656 \tTraining Loss: 0.009844\n",
            "Epoch: 657 \tTraining Loss: 0.009844\n",
            "Epoch: 658 \tTraining Loss: 0.009844\n",
            "Epoch: 659 \tTraining Loss: 0.009843\n",
            "Epoch: 660 \tTraining Loss: 0.009843\n",
            "Epoch: 661 \tTraining Loss: 0.009843\n",
            "Epoch: 662 \tTraining Loss: 0.009842\n",
            "Epoch: 663 \tTraining Loss: 0.009842\n",
            "Epoch: 664 \tTraining Loss: 0.009842\n",
            "Epoch: 665 \tTraining Loss: 0.009841\n",
            "Epoch: 666 \tTraining Loss: 0.009841\n",
            "Epoch: 667 \tTraining Loss: 0.009841\n",
            "Epoch: 668 \tTraining Loss: 0.009841\n",
            "Epoch: 669 \tTraining Loss: 0.009840\n",
            "Epoch: 670 \tTraining Loss: 0.009840\n",
            "Epoch: 671 \tTraining Loss: 0.009840\n",
            "Epoch: 672 \tTraining Loss: 0.009839\n",
            "Epoch: 673 \tTraining Loss: 0.009839\n",
            "Epoch: 674 \tTraining Loss: 0.009839\n",
            "Epoch: 675 \tTraining Loss: 0.009838\n",
            "Epoch: 676 \tTraining Loss: 0.009838\n",
            "Epoch: 677 \tTraining Loss: 0.009838\n",
            "Epoch: 678 \tTraining Loss: 0.009837\n",
            "Epoch: 679 \tTraining Loss: 0.009837\n",
            "Epoch: 680 \tTraining Loss: 0.009837\n",
            "Epoch: 681 \tTraining Loss: 0.009836\n",
            "Epoch: 682 \tTraining Loss: 0.009836\n",
            "Epoch: 683 \tTraining Loss: 0.009836\n",
            "Epoch: 684 \tTraining Loss: 0.009836\n",
            "Epoch: 685 \tTraining Loss: 0.009835\n",
            "Epoch: 686 \tTraining Loss: 0.009835\n",
            "Epoch: 687 \tTraining Loss: 0.009835\n",
            "Epoch: 688 \tTraining Loss: 0.009834\n",
            "Epoch: 689 \tTraining Loss: 0.009834\n",
            "Epoch: 690 \tTraining Loss: 0.009834\n",
            "Epoch: 691 \tTraining Loss: 0.009833\n",
            "Epoch: 692 \tTraining Loss: 0.009833\n",
            "Epoch: 693 \tTraining Loss: 0.009833\n",
            "Epoch: 694 \tTraining Loss: 0.009832\n",
            "Epoch: 695 \tTraining Loss: 0.009832\n",
            "Epoch: 696 \tTraining Loss: 0.009832\n",
            "Epoch: 697 \tTraining Loss: 0.009832\n",
            "Epoch: 698 \tTraining Loss: 0.009831\n",
            "Epoch: 699 \tTraining Loss: 0.009831\n",
            "Epoch: 700 \tTraining Loss: 0.009831\n",
            "Epoch: 701 \tTraining Loss: 0.009830\n",
            "Epoch: 702 \tTraining Loss: 0.009830\n",
            "Epoch: 703 \tTraining Loss: 0.009830\n",
            "Epoch: 704 \tTraining Loss: 0.009829\n",
            "Epoch: 705 \tTraining Loss: 0.009829\n",
            "Epoch: 706 \tTraining Loss: 0.009829\n",
            "Epoch: 707 \tTraining Loss: 0.009828\n",
            "Epoch: 708 \tTraining Loss: 0.009828\n",
            "Epoch: 709 \tTraining Loss: 0.009828\n",
            "Epoch: 710 \tTraining Loss: 0.009828\n",
            "Epoch: 711 \tTraining Loss: 0.009827\n",
            "Epoch: 712 \tTraining Loss: 0.009827\n",
            "Epoch: 713 \tTraining Loss: 0.009827\n",
            "Epoch: 714 \tTraining Loss: 0.009826\n",
            "Epoch: 715 \tTraining Loss: 0.009826\n",
            "Epoch: 716 \tTraining Loss: 0.009826\n",
            "Epoch: 717 \tTraining Loss: 0.009825\n",
            "Epoch: 718 \tTraining Loss: 0.009825\n",
            "Epoch: 719 \tTraining Loss: 0.009825\n",
            "Epoch: 720 \tTraining Loss: 0.009824\n",
            "Epoch: 721 \tTraining Loss: 0.009824\n",
            "Epoch: 722 \tTraining Loss: 0.009824\n",
            "Epoch: 723 \tTraining Loss: 0.009824\n",
            "Epoch: 724 \tTraining Loss: 0.009823\n",
            "Epoch: 725 \tTraining Loss: 0.009823\n",
            "Epoch: 726 \tTraining Loss: 0.009823\n",
            "Epoch: 727 \tTraining Loss: 0.009822\n",
            "Epoch: 728 \tTraining Loss: 0.009822\n",
            "Epoch: 729 \tTraining Loss: 0.009822\n",
            "Epoch: 730 \tTraining Loss: 0.009821\n",
            "Epoch: 731 \tTraining Loss: 0.009821\n",
            "Epoch: 732 \tTraining Loss: 0.009821\n",
            "Epoch: 733 \tTraining Loss: 0.009821\n",
            "Epoch: 734 \tTraining Loss: 0.009820\n",
            "Epoch: 735 \tTraining Loss: 0.009820\n",
            "Epoch: 736 \tTraining Loss: 0.009820\n",
            "Epoch: 737 \tTraining Loss: 0.009819\n",
            "Epoch: 738 \tTraining Loss: 0.009819\n",
            "Epoch: 739 \tTraining Loss: 0.009819\n",
            "Epoch: 740 \tTraining Loss: 0.009818\n",
            "Epoch: 741 \tTraining Loss: 0.009818\n",
            "Epoch: 742 \tTraining Loss: 0.009818\n",
            "Epoch: 743 \tTraining Loss: 0.009818\n",
            "Epoch: 744 \tTraining Loss: 0.009817\n",
            "Epoch: 745 \tTraining Loss: 0.009817\n",
            "Epoch: 746 \tTraining Loss: 0.009817\n",
            "Epoch: 747 \tTraining Loss: 0.009816\n",
            "Epoch: 748 \tTraining Loss: 0.009816\n",
            "Epoch: 749 \tTraining Loss: 0.009816\n",
            "Epoch: 750 \tTraining Loss: 0.009815\n",
            "Epoch: 751 \tTraining Loss: 0.009815\n",
            "Epoch: 752 \tTraining Loss: 0.009815\n",
            "Epoch: 753 \tTraining Loss: 0.009815\n",
            "Epoch: 754 \tTraining Loss: 0.009814\n",
            "Epoch: 755 \tTraining Loss: 0.009814\n",
            "Epoch: 756 \tTraining Loss: 0.009814\n",
            "Epoch: 757 \tTraining Loss: 0.009813\n",
            "Epoch: 758 \tTraining Loss: 0.009813\n",
            "Epoch: 759 \tTraining Loss: 0.009813\n",
            "Epoch: 760 \tTraining Loss: 0.009812\n",
            "Epoch: 761 \tTraining Loss: 0.009812\n",
            "Epoch: 762 \tTraining Loss: 0.009812\n",
            "Epoch: 763 \tTraining Loss: 0.009812\n",
            "Epoch: 764 \tTraining Loss: 0.009811\n",
            "Epoch: 765 \tTraining Loss: 0.009811\n",
            "Epoch: 766 \tTraining Loss: 0.009811\n",
            "Epoch: 767 \tTraining Loss: 0.009810\n",
            "Epoch: 768 \tTraining Loss: 0.009810\n",
            "Epoch: 769 \tTraining Loss: 0.009810\n",
            "Epoch: 770 \tTraining Loss: 0.009810\n",
            "Epoch: 771 \tTraining Loss: 0.009809\n",
            "Epoch: 772 \tTraining Loss: 0.009809\n",
            "Epoch: 773 \tTraining Loss: 0.009809\n",
            "Epoch: 774 \tTraining Loss: 0.009808\n",
            "Epoch: 775 \tTraining Loss: 0.009808\n",
            "Epoch: 776 \tTraining Loss: 0.009808\n",
            "Epoch: 777 \tTraining Loss: 0.009807\n",
            "Epoch: 778 \tTraining Loss: 0.009807\n",
            "Epoch: 779 \tTraining Loss: 0.009807\n",
            "Epoch: 780 \tTraining Loss: 0.009807\n",
            "Epoch: 781 \tTraining Loss: 0.009806\n",
            "Epoch: 782 \tTraining Loss: 0.009806\n",
            "Epoch: 783 \tTraining Loss: 0.009806\n",
            "Epoch: 784 \tTraining Loss: 0.009805\n",
            "Epoch: 785 \tTraining Loss: 0.009805\n",
            "Epoch: 786 \tTraining Loss: 0.009805\n",
            "Epoch: 787 \tTraining Loss: 0.009805\n",
            "Epoch: 788 \tTraining Loss: 0.009804\n",
            "Epoch: 789 \tTraining Loss: 0.009804\n",
            "Epoch: 790 \tTraining Loss: 0.009804\n",
            "Epoch: 791 \tTraining Loss: 0.009803\n",
            "Epoch: 792 \tTraining Loss: 0.009803\n",
            "Epoch: 793 \tTraining Loss: 0.009803\n",
            "Epoch: 794 \tTraining Loss: 0.009802\n",
            "Epoch: 795 \tTraining Loss: 0.009802\n",
            "Epoch: 796 \tTraining Loss: 0.009802\n",
            "Epoch: 797 \tTraining Loss: 0.009802\n",
            "Epoch: 798 \tTraining Loss: 0.009801\n",
            "Epoch: 799 \tTraining Loss: 0.009801\n",
            "Epoch: 800 \tTraining Loss: 0.009801\n",
            "Epoch: 801 \tTraining Loss: 0.009800\n",
            "Epoch: 802 \tTraining Loss: 0.009800\n",
            "Epoch: 803 \tTraining Loss: 0.009800\n",
            "Epoch: 804 \tTraining Loss: 0.009800\n",
            "Epoch: 805 \tTraining Loss: 0.009799\n",
            "Epoch: 806 \tTraining Loss: 0.009799\n",
            "Epoch: 807 \tTraining Loss: 0.009799\n",
            "Epoch: 808 \tTraining Loss: 0.009798\n",
            "Epoch: 809 \tTraining Loss: 0.009798\n",
            "Epoch: 810 \tTraining Loss: 0.009798\n",
            "Epoch: 811 \tTraining Loss: 0.009798\n",
            "Epoch: 812 \tTraining Loss: 0.009797\n",
            "Epoch: 813 \tTraining Loss: 0.009797\n",
            "Epoch: 814 \tTraining Loss: 0.009797\n",
            "Epoch: 815 \tTraining Loss: 0.009796\n",
            "Epoch: 816 \tTraining Loss: 0.009796\n",
            "Epoch: 817 \tTraining Loss: 0.009796\n",
            "Epoch: 818 \tTraining Loss: 0.009796\n",
            "Epoch: 819 \tTraining Loss: 0.009795\n",
            "Epoch: 820 \tTraining Loss: 0.009795\n",
            "Epoch: 821 \tTraining Loss: 0.009795\n",
            "Epoch: 822 \tTraining Loss: 0.009794\n",
            "Epoch: 823 \tTraining Loss: 0.009794\n",
            "Epoch: 824 \tTraining Loss: 0.009794\n",
            "Epoch: 825 \tTraining Loss: 0.009793\n",
            "Epoch: 826 \tTraining Loss: 0.009793\n",
            "Epoch: 827 \tTraining Loss: 0.009793\n",
            "Epoch: 828 \tTraining Loss: 0.009793\n",
            "Epoch: 829 \tTraining Loss: 0.009792\n",
            "Epoch: 830 \tTraining Loss: 0.009792\n",
            "Epoch: 831 \tTraining Loss: 0.009792\n",
            "Epoch: 832 \tTraining Loss: 0.009791\n",
            "Epoch: 833 \tTraining Loss: 0.009791\n",
            "Epoch: 834 \tTraining Loss: 0.009791\n",
            "Epoch: 835 \tTraining Loss: 0.009791\n",
            "Epoch: 836 \tTraining Loss: 0.009790\n",
            "Epoch: 837 \tTraining Loss: 0.009790\n",
            "Epoch: 838 \tTraining Loss: 0.009790\n",
            "Epoch: 839 \tTraining Loss: 0.009789\n",
            "Epoch: 840 \tTraining Loss: 0.009789\n",
            "Epoch: 841 \tTraining Loss: 0.009789\n",
            "Epoch: 842 \tTraining Loss: 0.009789\n",
            "Epoch: 843 \tTraining Loss: 0.009788\n",
            "Epoch: 844 \tTraining Loss: 0.009788\n",
            "Epoch: 845 \tTraining Loss: 0.009788\n",
            "Epoch: 846 \tTraining Loss: 0.009788\n",
            "Epoch: 847 \tTraining Loss: 0.009787\n",
            "Epoch: 848 \tTraining Loss: 0.009787\n",
            "Epoch: 849 \tTraining Loss: 0.009787\n",
            "Epoch: 850 \tTraining Loss: 0.009786\n",
            "Epoch: 851 \tTraining Loss: 0.009786\n",
            "Epoch: 852 \tTraining Loss: 0.009786\n",
            "Epoch: 853 \tTraining Loss: 0.009786\n",
            "Epoch: 854 \tTraining Loss: 0.009785\n",
            "Epoch: 855 \tTraining Loss: 0.009785\n",
            "Epoch: 856 \tTraining Loss: 0.009785\n",
            "Epoch: 857 \tTraining Loss: 0.009784\n",
            "Epoch: 858 \tTraining Loss: 0.009784\n",
            "Epoch: 859 \tTraining Loss: 0.009784\n",
            "Epoch: 860 \tTraining Loss: 0.009784\n",
            "Epoch: 861 \tTraining Loss: 0.009783\n",
            "Epoch: 862 \tTraining Loss: 0.009783\n",
            "Epoch: 863 \tTraining Loss: 0.009783\n",
            "Epoch: 864 \tTraining Loss: 0.009782\n",
            "Epoch: 865 \tTraining Loss: 0.009782\n",
            "Epoch: 866 \tTraining Loss: 0.009782\n",
            "Epoch: 867 \tTraining Loss: 0.009782\n",
            "Epoch: 868 \tTraining Loss: 0.009781\n",
            "Epoch: 869 \tTraining Loss: 0.009781\n",
            "Epoch: 870 \tTraining Loss: 0.009781\n",
            "Epoch: 871 \tTraining Loss: 0.009780\n",
            "Epoch: 872 \tTraining Loss: 0.009780\n",
            "Epoch: 873 \tTraining Loss: 0.009780\n",
            "Epoch: 874 \tTraining Loss: 0.009780\n",
            "Epoch: 875 \tTraining Loss: 0.009779\n",
            "Epoch: 876 \tTraining Loss: 0.009779\n",
            "Epoch: 877 \tTraining Loss: 0.009779\n",
            "Epoch: 878 \tTraining Loss: 0.009778\n",
            "Epoch: 879 \tTraining Loss: 0.009778\n",
            "Epoch: 880 \tTraining Loss: 0.009778\n",
            "Epoch: 881 \tTraining Loss: 0.009778\n",
            "Epoch: 882 \tTraining Loss: 0.009777\n",
            "Epoch: 883 \tTraining Loss: 0.009777\n",
            "Epoch: 884 \tTraining Loss: 0.009777\n",
            "Epoch: 885 \tTraining Loss: 0.009777\n",
            "Epoch: 886 \tTraining Loss: 0.009776\n",
            "Epoch: 887 \tTraining Loss: 0.009776\n",
            "Epoch: 888 \tTraining Loss: 0.009776\n",
            "Epoch: 889 \tTraining Loss: 0.009775\n",
            "Epoch: 890 \tTraining Loss: 0.009775\n",
            "Epoch: 891 \tTraining Loss: 0.009775\n",
            "Epoch: 892 \tTraining Loss: 0.009775\n",
            "Epoch: 893 \tTraining Loss: 0.009774\n",
            "Epoch: 894 \tTraining Loss: 0.009774\n",
            "Epoch: 895 \tTraining Loss: 0.009774\n",
            "Epoch: 896 \tTraining Loss: 0.009773\n",
            "Epoch: 897 \tTraining Loss: 0.009773\n",
            "Epoch: 898 \tTraining Loss: 0.009773\n",
            "Epoch: 899 \tTraining Loss: 0.009773\n",
            "Epoch: 900 \tTraining Loss: 0.009772\n",
            "Epoch: 901 \tTraining Loss: 0.009772\n",
            "Epoch: 902 \tTraining Loss: 0.009772\n",
            "Epoch: 903 \tTraining Loss: 0.009772\n",
            "Epoch: 904 \tTraining Loss: 0.009771\n",
            "Epoch: 905 \tTraining Loss: 0.009771\n",
            "Epoch: 906 \tTraining Loss: 0.009771\n",
            "Epoch: 907 \tTraining Loss: 0.009770\n",
            "Epoch: 908 \tTraining Loss: 0.009770\n",
            "Epoch: 909 \tTraining Loss: 0.009770\n",
            "Epoch: 910 \tTraining Loss: 0.009770\n",
            "Epoch: 911 \tTraining Loss: 0.009769\n",
            "Epoch: 912 \tTraining Loss: 0.009769\n",
            "Epoch: 913 \tTraining Loss: 0.009769\n",
            "Epoch: 914 \tTraining Loss: 0.009768\n",
            "Epoch: 915 \tTraining Loss: 0.009768\n",
            "Epoch: 916 \tTraining Loss: 0.009768\n",
            "Epoch: 917 \tTraining Loss: 0.009768\n",
            "Epoch: 918 \tTraining Loss: 0.009767\n",
            "Epoch: 919 \tTraining Loss: 0.009767\n",
            "Epoch: 920 \tTraining Loss: 0.009767\n",
            "Epoch: 921 \tTraining Loss: 0.009767\n",
            "Epoch: 922 \tTraining Loss: 0.009766\n",
            "Epoch: 923 \tTraining Loss: 0.009766\n",
            "Epoch: 924 \tTraining Loss: 0.009766\n",
            "Epoch: 925 \tTraining Loss: 0.009765\n",
            "Epoch: 926 \tTraining Loss: 0.009765\n",
            "Epoch: 927 \tTraining Loss: 0.009765\n",
            "Epoch: 928 \tTraining Loss: 0.009765\n",
            "Epoch: 929 \tTraining Loss: 0.009764\n",
            "Epoch: 930 \tTraining Loss: 0.009764\n",
            "Epoch: 931 \tTraining Loss: 0.009764\n",
            "Epoch: 932 \tTraining Loss: 0.009764\n",
            "Epoch: 933 \tTraining Loss: 0.009763\n",
            "Epoch: 934 \tTraining Loss: 0.009763\n",
            "Epoch: 935 \tTraining Loss: 0.009763\n",
            "Epoch: 936 \tTraining Loss: 0.009762\n",
            "Epoch: 937 \tTraining Loss: 0.009762\n",
            "Epoch: 938 \tTraining Loss: 0.009762\n",
            "Epoch: 939 \tTraining Loss: 0.009762\n",
            "Epoch: 940 \tTraining Loss: 0.009761\n",
            "Epoch: 941 \tTraining Loss: 0.009761\n",
            "Epoch: 942 \tTraining Loss: 0.009761\n",
            "Epoch: 943 \tTraining Loss: 0.009761\n",
            "Epoch: 944 \tTraining Loss: 0.009760\n",
            "Epoch: 945 \tTraining Loss: 0.009760\n",
            "Epoch: 946 \tTraining Loss: 0.009760\n",
            "Epoch: 947 \tTraining Loss: 0.009759\n",
            "Epoch: 948 \tTraining Loss: 0.009759\n",
            "Epoch: 949 \tTraining Loss: 0.009759\n",
            "Epoch: 950 \tTraining Loss: 0.009759\n",
            "Epoch: 951 \tTraining Loss: 0.009758\n",
            "Epoch: 952 \tTraining Loss: 0.009758\n",
            "Epoch: 953 \tTraining Loss: 0.009758\n",
            "Epoch: 954 \tTraining Loss: 0.009758\n",
            "Epoch: 955 \tTraining Loss: 0.009757\n",
            "Epoch: 956 \tTraining Loss: 0.009757\n",
            "Epoch: 957 \tTraining Loss: 0.009757\n",
            "Epoch: 958 \tTraining Loss: 0.009756\n",
            "Epoch: 959 \tTraining Loss: 0.009756\n",
            "Epoch: 960 \tTraining Loss: 0.009756\n",
            "Epoch: 961 \tTraining Loss: 0.009756\n",
            "Epoch: 962 \tTraining Loss: 0.009755\n",
            "Epoch: 963 \tTraining Loss: 0.009755\n",
            "Epoch: 964 \tTraining Loss: 0.009755\n",
            "Epoch: 965 \tTraining Loss: 0.009755\n",
            "Epoch: 966 \tTraining Loss: 0.009754\n",
            "Epoch: 967 \tTraining Loss: 0.009754\n",
            "Epoch: 968 \tTraining Loss: 0.009754\n",
            "Epoch: 969 \tTraining Loss: 0.009753\n",
            "Epoch: 970 \tTraining Loss: 0.009753\n",
            "Epoch: 971 \tTraining Loss: 0.009753\n",
            "Epoch: 972 \tTraining Loss: 0.009753\n",
            "Epoch: 973 \tTraining Loss: 0.009752\n",
            "Epoch: 974 \tTraining Loss: 0.009752\n",
            "Epoch: 975 \tTraining Loss: 0.009752\n",
            "Epoch: 976 \tTraining Loss: 0.009752\n",
            "Epoch: 977 \tTraining Loss: 0.009751\n",
            "Epoch: 978 \tTraining Loss: 0.009751\n",
            "Epoch: 979 \tTraining Loss: 0.009751\n",
            "Epoch: 980 \tTraining Loss: 0.009751\n",
            "Epoch: 981 \tTraining Loss: 0.009750\n",
            "Epoch: 982 \tTraining Loss: 0.009750\n",
            "Epoch: 983 \tTraining Loss: 0.009750\n",
            "Epoch: 984 \tTraining Loss: 0.009749\n",
            "Epoch: 985 \tTraining Loss: 0.009749\n",
            "Epoch: 986 \tTraining Loss: 0.009749\n",
            "Epoch: 987 \tTraining Loss: 0.009749\n",
            "Epoch: 988 \tTraining Loss: 0.009748\n",
            "Epoch: 989 \tTraining Loss: 0.009748\n",
            "Epoch: 990 \tTraining Loss: 0.009748\n",
            "Epoch: 991 \tTraining Loss: 0.009748\n",
            "Epoch: 992 \tTraining Loss: 0.009747\n",
            "Epoch: 993 \tTraining Loss: 0.009747\n",
            "Epoch: 994 \tTraining Loss: 0.009747\n",
            "Epoch: 995 \tTraining Loss: 0.009747\n",
            "Epoch: 996 \tTraining Loss: 0.009746\n",
            "Epoch: 997 \tTraining Loss: 0.009746\n",
            "Epoch: 998 \tTraining Loss: 0.009746\n",
            "Epoch: 999 \tTraining Loss: 0.009745\n",
            "Epoch: 1000 \tTraining Loss: 0.009745\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "output=model(xx1)\n",
        "s=0\n",
        "for i in range (len(y_test)) :\n",
        "  s+= (yy1[i]-output[i])**2\n",
        "print(s/len(y_test))\n",
        "print(s)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p-3U74844UKB",
        "outputId": "1f5bf5f3-edc2-4c92-c3fb-2b06f2c2d75e"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([0.0076], grad_fn=<DivBackward0>)\n",
            "tensor([0.7704], grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
            "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
          ]
        }
      ]
    }
  ]
}