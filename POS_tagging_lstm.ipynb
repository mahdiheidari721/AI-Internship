{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNkKmOXr1vm/oB9t211IoQL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mahdiheidari721/AI-Internship/blob/master/POS_tagging_lstm.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SFRfkCvS-bNs"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "from nltk.corpus import brown\n",
        "from nltk.corpus import treebank\n",
        "from nltk.corpus import conll2000\n",
        "\n",
        "import seaborn as sns\n",
        "\n",
        "from gensim.models import KeyedVectors\n",
        "\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.utils.np_utils import to_categorical\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Embedding\n",
        "from keras.layers import Dense, Input\n",
        "from keras.layers import TimeDistributed\n",
        "from keras.layers import LSTM, GRU, Bidirectional, SimpleRNN, RNN\n",
        "from keras.models import Model\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.utils import shuffle\n",
        "import nltk\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "072byh13VXOf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1b736696-c538-45a7-e580-f592eb5d1c5e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/treebank.zip.\n",
            "[nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/brown.zip.\n",
            "[nltk_data] Downloading package conll2000 to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/conll2000.zip.\n",
            "[nltk_data] Downloading package universal_tagset to /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/universal_tagset.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "nltk.download('treebank')\n",
        "nltk.download('brown')\n",
        "nltk.download('conll2000')\n",
        "nltk.download('universal_tagset')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PKmncnLPSyRv"
      },
      "outputs": [],
      "source": [
        "# load POS tagged corpora from NLTK\n",
        "treebank_corpus = treebank.tagged_sents(tagset='universal')\n",
        "brown_corpus = brown.tagged_sents(tagset='universal')\n",
        "conll_corpus = conll2000.tagged_sents(tagset='universal')\n",
        "#tagged_sentence = treebank_corpus + brown_corpus + conll_corpus\n",
        "tagged_sentence = treebank_corpus "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RS_ob8h9Vj-t",
        "outputId": "ba9d7dba-4940-48bc-9235-baedd4bfbe8a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('A', 'DET'), ('Lorillard', 'NOUN'), ('spokewoman', 'NOUN'), ('said', 'VERB'), (',', '.'), ('``', '.'), ('This', 'DET'), ('is', 'VERB'), ('an', 'DET'), ('old', 'ADJ'), ('story', 'NOUN'), ('.', '.')]\n"
          ]
        }
      ],
      "source": [
        "print(tagged_sentence[7])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AlixKK2oWEhZ"
      },
      "outputs": [],
      "source": [
        "X = [] # store input sequence\n",
        "Y = [] # store output sequence\n",
        "\n",
        "for sentence in tagged_sentence:\n",
        "    X_sentence = []\n",
        "    Y_sentence = []\n",
        "    for entity in sentence:         \n",
        "        X_sentence.append(entity[0])  # entity[0] contains the word\n",
        "        Y_sentence.append(entity[1])  # entity[1] contains corresponding tag\n",
        "        \n",
        "    X.append(X_sentence)\n",
        "    Y.append(Y_sentence)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Sentence=[]\n",
        "for i in range(len(X)):\n",
        "  Sentence.append((X[i],Y[i]))\n",
        "\n"
      ],
      "metadata": {
        "id": "EVdTvzi2Rom4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def word_to_ix(word, ix):\n",
        "    return torch.tensor(ix[word], dtype = torch.long)\n",
        "\n",
        "def char_to_ix(char, ix):\n",
        "    return torch.tensor(ix[char], dtype= torch.long)\n",
        "\n",
        "def tag_to_ix(tag, ix):\n",
        "    return torch.tensor(ix[tag], dtype= torch.long)\n",
        "\n",
        "def sequence_to_idx(sequence, ix):\n",
        "    return torch.tensor([ix[s] for s in sequence], dtype=torch.long)\n",
        "\n",
        "\n",
        "word_to_idx = {}\n",
        "tag_to_idx = {}\n",
        "char_to_idx = {}\n",
        "for sentence in tagged_sentence:\n",
        "    for word, pos_tag in sentence:\n",
        "        if word not in word_to_idx.keys():\n",
        "            word_to_idx[word] = len(word_to_idx)\n",
        "        if pos_tag not in tag_to_idx.keys():\n",
        "            tag_to_idx[pos_tag] = len(tag_to_idx)\n",
        "        for char in word:\n",
        "            if char not in char_to_idx.keys():\n",
        "                char_to_idx[char] = len(char_to_idx)"
      ],
      "metadata": {
        "id": "YUwODIwvTk65"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word_vocab_size = len(word_to_idx)\n",
        "tag_vocab_size = len(tag_to_idx)\n",
        "char_vocab_size = len(char_to_idx)\n",
        "\n",
        "print(\"Unique words: {}\".format(len(word_to_idx)))\n",
        "print(\"Unique tags: {}\".format(len(tag_to_idx)))\n",
        "print(\"Unique characters: {}\".format(len(char_to_idx)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V4Wlryp8V_5-",
        "outputId": "cb78d23b-bbc1-4a97-a49d-7b6d6a7688db"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unique words: 12408\n",
            "Unique tags: 12\n",
            "Unique characters: 79\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "tr_random = random.sample(list(range(len(tagged_sentence))), int(0.95 * len(tagged_sentence)))\n",
        "\n",
        "train = [tagged_sentence[i] for i in tr_random]\n",
        "test = [tagged_sentence[i] for i in range(len(tagged_sentence)) if i not in tr_random]"
      ],
      "metadata": {
        "id": "uppLz9RBROLM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "WORD_EMBEDDING_DIM = 1024\n",
        "CHAR_EMBEDDING_DIM = 128\n",
        "WORD_HIDDEN_DIM = 1024\n",
        "CHAR_HIDDEN_DIM = 1024\n",
        "EPOCHS = 70"
      ],
      "metadata": {
        "id": "cmlZhG-2PCW9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DualLSTMTagger(nn.Module):\n",
        "    def __init__(self, word_embedding_dim, word_hidden_dim, char_embedding_dim, char_hidden_dim, word_vocab_size, char_vocab_size, tag_vocab_size):\n",
        "        super(DualLSTMTagger, self).__init__()\n",
        "        self.word_embedding = nn.Embedding(word_vocab_size, word_embedding_dim)\n",
        "        \n",
        "        self.char_embedding = nn.Embedding(char_vocab_size, char_embedding_dim)\n",
        "        self.char_lstm = nn.LSTM(char_embedding_dim, char_hidden_dim)\n",
        "        \n",
        "        self.lstm = nn.LSTM(word_embedding_dim + char_hidden_dim, word_hidden_dim)\n",
        "        self.hidden2tag = nn.Linear(word_hidden_dim, tag_vocab_size)\n",
        "        \n",
        "    def forward(self, sentence, words):\n",
        "        embeds = self.word_embedding(sentence)\n",
        "        char_hidden_final = []\n",
        "        for word in words:\n",
        "            char_embeds = self.char_embedding(word)\n",
        "            _, (char_hidden, char_cell_state) = self.char_lstm(char_embeds.view(len(word), 1, -1))\n",
        "            word_char_hidden_state = char_hidden.view(-1)\n",
        "            char_hidden_final.append(word_char_hidden_state)\n",
        "        char_hidden_final = torch.stack(tuple(char_hidden_final))\n",
        "        \n",
        "        combined = torch.cat((embeds, char_hidden_final), 1)\n",
        "\n",
        "        lstm_out, _ = self.lstm(combined.view(len(sentence), 1, -1))\n",
        "        tag_space = self.hidden2tag(lstm_out.view(len(sentence), -1))\n",
        "        \n",
        "        tag_scores = F.log_softmax(tag_space, dim=1)\n",
        "        return tag_scores"
      ],
      "metadata": {
        "id": "rp-axYU3PtZV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = DualLSTMTagger(WORD_EMBEDDING_DIM, WORD_HIDDEN_DIM, CHAR_EMBEDDING_DIM, CHAR_HIDDEN_DIM, word_vocab_size, char_vocab_size, tag_vocab_size)\n",
        "\n",
        "use_cuda = torch.cuda.is_available()\n",
        "device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
        "\n",
        "if use_cuda:\n",
        "    model.cuda()\n",
        "\n",
        "# Define the loss function as the Negative Log Likelihood loss (NLLLoss)\n",
        "loss_function = nn.NLLLoss()\n",
        "\n",
        "# We will be using a simple SGD optimizer\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
        "\n",
        "# The test sentence\n",
        "seq = \"everybody eat the food . I kept looking out the window , trying to find the one I was waiting for .\".split()\n",
        "print(\"Running a check on the model before training.\\nSentences:\\n{}\".format(\" \".join(seq)))\n",
        "with torch.no_grad():\n",
        "    words = [torch.tensor(sequence_to_idx(s[0], char_to_idx), dtype=torch.long).to(device) for s in seq]\n",
        "    sentence = torch.tensor(sequence_to_idx(seq, word_to_idx), dtype=torch.long).to(device)\n",
        "        \n",
        "    tag_scores = model(sentence, words)\n",
        "    _, indices = torch.max(tag_scores, 1)\n",
        "    ret = []\n",
        "    for i in range(len(indices)):\n",
        "        for key, value in tag_to_idx.items():\n",
        "            if indices[i] == value:\n",
        "                ret.append((seq[i], key))\n",
        "    print(ret)\n",
        "# Training start\n",
        "print(\"Training Started\")\n",
        "accuracy_list = []\n",
        "loss_list = []\n",
        "interval = round(len(train) / 100.)\n",
        "epochs = EPOCHS\n",
        "e_interval = round(epochs / 10.)\n",
        "for epoch in range(epochs):\n",
        "    acc = 0 #to keep track of accuracy\n",
        "    loss = 0 # To keep track of the loss value\n",
        "    i = 0\n",
        "    for sentence_tag in train:\n",
        "        i += 1\n",
        "        words = [torch.tensor(sequence_to_idx(s[0], char_to_idx), dtype=torch.long).to(device) for s in sentence_tag]\n",
        "        sentence = [s[0] for s in sentence_tag]\n",
        "        sentence = torch.tensor(sequence_to_idx(sentence, word_to_idx), dtype=torch.long).to(device)\n",
        "        targets = [s[1] for s in sentence_tag]\n",
        "        targets = torch.tensor(sequence_to_idx(targets, tag_to_idx), dtype=torch.long).to(device)\n",
        "        \n",
        "        model.zero_grad()\n",
        "        \n",
        "        tag_scores = model(sentence, words)\n",
        "        \n",
        "        loss = loss_function(tag_scores, targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        loss += loss.item()\n",
        "        _, indices = torch.max(tag_scores, 1)\n",
        "#         print(indices == targets)\n",
        "        acc += torch.mean(torch.tensor(targets == indices, dtype=torch.float))\n",
        "        if i % interval == 0:\n",
        "            print(\"Epoch {} Running;\\t{}% Complete\".format(epoch + 1, i / interval), end = \"\\r\", flush = True)\n",
        "    loss = loss / len(train)\n",
        "    acc = acc / len(train)\n",
        "    loss_list.append(float(loss))\n",
        "    accuracy_list.append(float(acc))\n",
        "    if (epoch + 1) % e_interval == 0:\n",
        "        print(\"Epoch {} Completed,\\tLoss {}\\tAccuracy: {}\".format(epoch + 1, np.mean(loss_list[-e_interval:]), np.mean(accuracy_list[-e_interval:])))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cgWXbhNnPqm8",
        "outputId": "a7872ffc-d38e-4b06-9a36-ffb89c68cd81"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running a check on the model before training.\n",
            "Sentences:\n",
            "everybody eat the food . I kept looking out the window , trying to find the one I was waiting for .\n",
            "[('everybody', 'ADP'), ('eat', 'ADP'), ('the', 'PRON'), ('food', 'NOUN'), ('.', 'NOUN'), ('I', 'NOUN'), ('kept', 'NOUN'), ('looking', 'ADV'), ('out', 'NOUN'), ('the', 'NOUN'), ('window', 'NOUN'), (',', 'NOUN'), ('trying', 'NOUN'), ('to', 'NUM'), ('find', 'NOUN'), ('the', 'NOUN'), ('one', 'ADP'), ('I', 'ADP'), ('was', 'PRT'), ('waiting', 'DET'), ('for', '.'), ('.', 'NUM')]\n",
            "Training Started\n",
            "Epoch 7 Completed,\tLoss 9.668207957875501e-05\tAccuracy: 0.9411884120532444\n",
            "Epoch 14 Completed,\tLoss 6.128648365146181e-05\tAccuracy: 0.9550539595740182\n",
            "Epoch 21 Completed,\tLoss 3.0193763032796458e-06\tAccuracy: 0.9578626411301749\n",
            "Epoch 28 Completed,\tLoss 3.7462546394830188e-06\tAccuracy: 0.9597723654338292\n",
            "Epoch 35 Completed,\tLoss 7.923165365523812e-06\tAccuracy: 0.9620928849492755\n",
            "Epoch 42 Completed,\tLoss 4.458149682751156e-10\tAccuracy: 0.9643318993704659\n",
            "Epoch 49 Completed,\tLoss 6.523452585428802e-05\tAccuracy: 0.9659421018191746\n",
            "Epoch 56 Completed,\tLoss 0.00012666582012315044\tAccuracy: 0.9676969562258039\n",
            "Epoch 63 Completed,\tLoss 6.445731078375181e-07\tAccuracy: 0.9683042083467756\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.plot(accuracy_list, c=\"red\", label =\"Accuracy\")\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Value\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "plt.plot(loss_list, c=\"blue\", label =\"Loss\")\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Value\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "s78Hrv3FP6Qb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad():\n",
        "    words = [torch.tensor(sequence_to_idx(s[0], char_to_idx), dtype=torch.long).to(device) for s in seq]\n",
        "    sentence = torch.tensor(sequence_to_idx(seq, word_to_idx), dtype=torch.long).to(device)\n",
        "        \n",
        "    tag_scores = model(sentence, words)\n",
        "    _, indices = torch.max(tag_scores, 1)\n",
        "    ret = []\n",
        "    for i in range(len(indices)):\n",
        "        for key, value in tag_to_idx.items():\n",
        "            if indices[i] == value:\n",
        "                ret.append((seq[i], key))\n",
        "    print(ret)"
      ],
      "metadata": {
        "id": "ZUyXd9w0QMsw"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}